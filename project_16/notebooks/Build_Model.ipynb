{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src') \n",
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Libraries for plotting curves\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from itertools import cycle\n",
    "\n",
    "# Importing script\n",
    "import etl as etl\n",
    "import visualize_data as visualize_data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disease Risk Classification Model\n",
    "\n",
    "The purpose of this notebook is to explore a variety of models and tune them in order to figure out which is best for disease risk classification. According to the article \"Comparing different supervised machine learning algorithms for disease prediction\" by Shahadat Uddin et al. (https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-1004-8), Support Vector Machines, Naive Bayes, and Random Forest are some of the most common machine learning algorithms applied to disease prediction. We will explore these along with other algorithms such as Logistic Regression and K Nearest Neighbors in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get straight into it and simulate a data set of 5,000 individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_gwas_fp = '../testdata/coronary_artery/EFO_0001645.csv' \n",
    "etl.simulate_data('.', 'simulated_data', simulated_gwas_fp, 1000)\n",
    "simulated_data = pd.read_csv('simulated_data.csv')\n",
    "simulated_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be using the simulated data above for building the model. Given that the class label (disease risk category) was assigned to an individual depending on the weighted sum of all and only the SNPs in this data set, it would be too easy for a machine learning model to figure this out. Essentially, the above data set is a simulated 'ground truth'. In machine learning problems you typically don't work with all the variables that determine the label so something needs to be done about this.\n",
    "\n",
    "As a solution, we will be making classifications based on a subset of SNPs in the above data set. To do this, we will be using a separate GWAS to inform us of SNPs that are most important in predicting disease. Only those SNPs that are in the above data set and the new GWAS will be used. The model will then be trained on this subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the other GWAS data and filter the above data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gwas_fp = '../testdata/coronary_artery/EFO_0000378.csv' \n",
    "model_data = pd.read_csv(model_gwas_fp)\n",
    "subset = set(simulated_data.columns).intersection(model_data['variant_id'].unique())\n",
    "new_columns = list(subset)+['Class']\n",
    "\n",
    "data = simulated_data[new_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a training and test set on the above data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get proportion of each class\n",
    "prop_per_class = y.value_counts(normalize=True)\n",
    "prop_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train different models on these sets and assess their results.\n",
    "\n",
    "**Note:**\n",
    "\n",
    ">It is important to note that accuracy is not the only metric that is important here to assess the performance of the models. Since we are working with predicting the disease risk of individuals, **Type I** and **Type II** errors are also very important. It is potentially dangerous to classify an individual as low risk when they are actually high risk (Type II, False Negative). Additionally, classifying an individual as high risk when they are actually low risk could cause the individual some unecessary stress within themselves and their families (Type I, False Positive). Therefore, it is important to control for such errors in our model. To do so, we will prioritize the maximization of **Recall** (TP/TP+FN) since it is an indicator of the dangerous False Negatives in our model but at the same time attempt to maximize **Precision** (TP/TP+FP) since it is an indicator of False Positives. \n",
    "\n",
    ">In general:\n",
    "    - Accuracy is a great metric only when you have symmetric datasets (meaning false negatives & false positives counts are close) and when false negatives & false positives have similar costs. If the cost of false positives and false negatives are different, then F1-score is the best option. F1-score is best if you have an uneven class distribution.\n",
    "    - Recall/Sensitivity is the best metric if the idea of false positives is far better than false negatives. In other words, if the occurrence of false negatives is unaccepted/intolerable, then you'd rather get some extra false positives (false alarms) over saving some false negatives, like in the diabetes example. In that example, you'd rather get some healthy people labeled diabetic over leaving a diabetic person labeled healthy.\n",
    "    - Precision is the best metric if you want to be more confident of your true positives. For example, with spam emails, you'd rather have some spam emails in your inbox rather than some regular emails in your spam box. So, the email company wants to be extra sure that email Y is spam before they put it in the spam box and you never get to see it.\n",
    "    - Specificity is the best metric if you want to cover all true negatives, meaning you don't want any false alarms (false positives). For example, if you're running a drug test in which all people who test positive will immediately go to jail, then you don't want anyone drug-free going to jail. In this example, false positives are intolerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)\n",
    "\n",
    "preds_lg = lg.predict(X_test)\n",
    "accuracy_lg = lg.score(X_test, y_test)*100\n",
    "print('The accuracy for the Logistic Regression model is {}%'.format(accuracy_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_lr = f1_score(y_test.values, preds_lg, average='weighted')\n",
    "print('f1-score:', f1_lr, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_lr = confusion_matrix(y_test.values, preds_lg)\n",
    "\n",
    "FP = cnf_matrix_lr.sum(axis=0) - np.diag(cnf_matrix_lr)  \n",
    "FN = cnf_matrix_lr.sum(axis=1) - np.diag(cnf_matrix_lr)\n",
    "TP = np.diag(cnf_matrix_lr)\n",
    "TN = cnf_matrix_lr.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_lr, tnr_medrisk_lr, tnr_lowrisk_lr = TNR\n",
    "print(tnr_highrisk_lr, tnr_medrisk_lr, tnr_lowrisk_lr)\n",
    "\n",
    "tnr_lr = np.mean(TNR)\n",
    "print('average specificity:', tnr_lr)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to improve the model using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_parameters = {'tol':[.1, .001, .0001], 'C':[10, 1, .1]}\n",
    "lg = LogisticRegression()\n",
    "clf1 = GridSearchCV(lg, lg_parameters)\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "preds_lr = clf1.predict(X_test)\n",
    "accuracy_lr = np.mean(y_test == preds_lr)*100\n",
    "\n",
    "\n",
    "print('The accuracy for the refined Logistic Regression model is {}%'.format(accuracy_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clf1.best_params_)\n",
    "# print(clf1.best_score_)\n",
    "# display(pd.DataFrame.from_dict(clf1.cv_results_).sort_values('rank_test_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "print(classification_report(y_test, preds_lr, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv = LogisticRegression()\n",
    "\n",
    "cv_scores_LR = cross_val_score(lr_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores_LR))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores_LR)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "params = {'tol':[.1, .0001, 1e-5], 'C':[10, 1, .1]}\n",
    "\n",
    "lr_gscv = GridSearchCV(lr, params, cv=5)\n",
    "\n",
    "lr_gscv.fit(X_train, y_train)\n",
    "preds_lr = lr_gscv.predict(X_test)\n",
    "accuracy_lr_gscv = np.mean(y_test == preds_lr)*100\n",
    "\n",
    "print('The accuracy for the refined Logistic Regression model is {}%'.format(accuracy_lr_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in lr_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined Logistic Regression model is {}%'.format(lr_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_lr_gscv = f1_score(y_test.values, preds_lr, average='weighted')\n",
    "print('f1-score with grid search:', f1_lr_gscv, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_lr_gscv = confusion_matrix(y_test.values, preds_lr)\n",
    "\n",
    "FP = cnf_matrix_lr_gscv.sum(axis=0) - np.diag(cnf_matrix_lr_gscv)  \n",
    "FN = cnf_matrix_lr_gscv.sum(axis=1) - np.diag(cnf_matrix_lr_gscv)\n",
    "TP = np.diag(cnf_matrix_lr_gscv)\n",
    "TN = cnf_matrix_lr_gscv.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_lr_gscv, tnr_medrisk_lr_gscv, tnr_lowrisk_lr_gscv = TNR\n",
    "print(tnr_highrisk_lr_gscv, tnr_medrisk_lr_gscv, tnr_lowrisk_lr_gscv)\n",
    "\n",
    "tnr_lr_gscv = np.mean(TNR)\n",
    "print('average specificity:', tnr_lr_gscv)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "lr_best = LogisticRegression(C=10, tol=0.0001)\n",
    "lr_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "lr_pr = visualize_data.plot_precision_recall(\n",
    "    'Logistic Regression', lr_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "lr_best = LogisticRegression(C=10, tol=0.0001)\n",
    "lr_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "lr_roc = visualize_data.plot_multiclass_roc(\n",
    "    'Logistic Regression', lr_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "preds_knn = knn.predict(X_test)\n",
    "accuracy_knn = knn.score(X_test, y_test)*100\n",
    "print('The accuracy for the K Nearest Neighbors model is {}%'.format(accuracy_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_knn = f1_score(y_test.values, preds_knn, average='weighted')\n",
    "print('f1-score:', f1_lr, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_knn = confusion_matrix(y_test.values, preds_knn)\n",
    "\n",
    "FP = cnf_matrix_knn.sum(axis=0) - np.diag(cnf_matrix_knn)  \n",
    "FN = cnf_matrix_knn.sum(axis=1) - np.diag(cnf_matrix_knn)\n",
    "TP = np.diag(cnf_matrix_knn)\n",
    "TN = cnf_matrix_knn.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_knn, tnr_medrisk_knn, tnr_lowrisk_knn = TNR\n",
    "print(tnr_highrisk_knn, tnr_medrisk_knn, tnr_lowrisk_knn)\n",
    "\n",
    "tnr_knn = np.mean(TNR)\n",
    "print('average specificity:', tnr_knn)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to improve the model using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_parameters = {'n_neighbors':[10, 5, 3, 1], 'p':[2, 1]}\n",
    "knn = KNeighborsClassifier()\n",
    "clf2 = GridSearchCV(knn, knn_parameters)\n",
    "\n",
    "clf2.fit(X_train, y_train)\n",
    "preds_knn = clf2.predict(X_test)\n",
    "accuracy_knn2 = np.mean(y_test == preds_knn)*100\n",
    "\n",
    "print('The accuracy for the refined K Nearest Neighbors model is {}%'.format(accuracy_knn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_knn, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv = KNeighborsClassifier()\n",
    "\n",
    "cv_scores = cross_val_score(knn_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "params = {'n_neighbors': [10, 5, 3, 1], 'p':[3, 2, 1]}\n",
    "\n",
    "knn_gscv = GridSearchCV(knn, params, cv=5)\n",
    "\n",
    "knn_gscv.fit(X_train, y_train)\n",
    "preds_knn = knn_gscv.predict(X_test)\n",
    "accuracy_knn_gscv = np.mean(y_test == preds_knn)*100\n",
    "\n",
    "print('The accuracy for the refined K Nearest Neighbors model is {}%'.format(accuracy_knn_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in knn_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined K Nearest Neighbors model is {}%'.format(knn_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_knn_gscv = f1_score(y_test.values, preds_knn, average='weighted')\n",
    "print('f1-score with grid search:', f1_knn_gscv, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_knn_gscv = confusion_matrix(y_test.values, preds_knn)\n",
    "\n",
    "FP = cnf_matrix_knn_gscv.sum(axis=0) - np.diag(cnf_matrix_knn_gscv)  \n",
    "FN = cnf_matrix_knn_gscv.sum(axis=1) - np.diag(cnf_matrix_knn_gscv)\n",
    "TP = np.diag(cnf_matrix_knn_gscv)\n",
    "TN = cnf_matrix_knn_gscv.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_knn_gscv, tnr_medrisk_knn_gscv, tnr_lowrisk_knn_gscv = TNR\n",
    "print(tnr_highrisk_knn_gscv, tnr_medrisk_knn_gscv, tnr_lowrisk_knn_gscv)\n",
    "\n",
    "tnr_knn_gscv = np.mean(TNR)\n",
    "print('average specificity:', tnr_knn_gscv)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "knn_best = KNeighborsClassifier(n_neighbors=3, p=3)\n",
    "knn_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'K Nearest Neighbors', knn_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "knn_best = KNeighborsClassifier(n_neighbors=3, p=3)\n",
    "knn_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'K Nearest Neighbors', knn_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "preds_svc = svc.predict(X_test)\n",
    "accuracy_svc = svc.score(X_test, y_test)*100\n",
    "print('The accuracy for the Support Vector Machine model is {}%'.format(accuracy_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_svc = f1_score(y_test.values, preds_svc, average='weighted')\n",
    "print('f1-score:', f1_svc, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_svc = confusion_matrix(y_test.values, preds_svc)\n",
    "\n",
    "FP = cnf_matrix_svc.sum(axis=0) - np.diag(cnf_matrix_svc)  \n",
    "FN = cnf_matrix_svc.sum(axis=1) - np.diag(cnf_matrix_svc)\n",
    "TP = np.diag(cnf_matrix_svc)\n",
    "TN = cnf_matrix_svc.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_svc, tnr_medrisk_svc, tnr_lowrisk_svc = TNR\n",
    "print(tnr_highrisk_svc, tnr_medrisk_svc, tnr_lowrisk_svc)\n",
    "\n",
    "tnr_svc = np.mean(TNR)\n",
    "print('average specificity:', tnr_svc)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_parameters = {'tol': [.1, .001, .0001], 'C':[10, 1, .1]}\n",
    "svc = SVC()\n",
    "clf3 = GridSearchCV(svc, svc_parameters)\n",
    "\n",
    "clf3.fit(X_train, y_train)\n",
    "preds_svc = clf3.predict(X_test)\n",
    "accuracy_svc2 = np.mean(y_test == preds_svc)*100\n",
    "\n",
    "print('The accuracy for the refined Support Vector Machine model is {}%'.format(accuracy_svc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_svc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_cv = SVC()\n",
    "\n",
    "cv_scores = cross_val_score(svc_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "params = {'tol': [.1, .001, .0001], 'C':[10, 1, .1]}\n",
    "svc_gscv = GridSearchCV(svc, params, cv=5)\n",
    "\n",
    "svc_gscv.fit(X_train, y_train)\n",
    "preds_svc = svc_gscv.predict(X_test)\n",
    "accuracy_svc_gscv = np.mean(y_test == preds_svc)*100\n",
    "\n",
    "print('The accuracy for the refined Support Vector Machine model is {}%'.format(accuracy_svc_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in svc_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined Support Vector Machine model is {}%'.format(svc_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_svc_gscv = f1_score(y_test.values, preds_svc, average='weighted')\n",
    "print('f1-score with grid search:', f1_svc_gscv, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_svc_gscv = confusion_matrix(y_test.values, preds_svc)\n",
    "\n",
    "FP = cnf_matrix_svc_gscv.sum(axis=0) - np.diag(cnf_matrix_svc_gscv)  \n",
    "FN = cnf_matrix_svc_gscv.sum(axis=1) - np.diag(cnf_matrix_svc_gscv)\n",
    "TP = np.diag(cnf_matrix_svc_gscv)\n",
    "TN = cnf_matrix_svc_gscv.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_svc_gscv, tnr_medrisk_svc_gscv, tnr_lowrisk_svc_gscv = TNR\n",
    "print(tnr_highrisk_svc_gscv, tnr_medrisk_svc_gscv, tnr_lowrisk_svc_gscv)\n",
    "\n",
    "tnr_svc_gscv = np.mean(TNR)\n",
    "print('average specificity:', tnr_svc_gscv)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "svc_best = SVC(C=10, tol=0.1)\n",
    "svc_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'Support Vector Machine', svc_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "svc_best = SVC(C=10, tol=0.1)\n",
    "svc_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'Support Vector Machine', svc_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "preds_gnb = gnb.predict(X_test)\n",
    "accuracy_gnb = gnb.score(X_test, y_test)*100\n",
    "print('The accuracy for the Naive Bayes is {}%'.format(accuracy_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_gnb = f1_score(y_test.values, preds_gnb, average='weighted')\n",
    "print('f1-score:', f1_gnb, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_gnb = confusion_matrix(y_test.values, preds_gnb)\n",
    "\n",
    "FP = cnf_matrix_gnb.sum(axis=0) - np.diag(cnf_matrix_gnb)  \n",
    "FN = cnf_matrix_gnb.sum(axis=1) - np.diag(cnf_matrix_gnb)\n",
    "TP = np.diag(cnf_matrix_gnb)\n",
    "TN = cnf_matrix_gnb.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_gnb, tnr_medrisk_gnb, tnr_lowrisk_gnb = TNR\n",
    "print(tnr_highrisk_gnb, tnr_medrisk_gnb, tnr_lowrisk_gnb)\n",
    "\n",
    "tnr_gnb = np.mean(TNR)\n",
    "print('average specificity:', tnr_gnb)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_parameters = {'var_smoothing':[1e-3, 1e-6, 1e-9]}\n",
    "gnb = GaussianNB()\n",
    "clf4 = GridSearchCV(gnb, gnb_parameters)\n",
    "\n",
    "clf4.fit(X_train, y_train)\n",
    "preds_gnb = clf4.predict(X_test)\n",
    "accuracy_gnb2 = np.mean(y_test == preds_gnb)*100\n",
    "\n",
    "print('The accuracy for the refined Naive Bayes model is {}%'.format(accuracy_gnb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_gnb, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-fold cross-validation\n",
    "gnb_cv = GaussianNB()\n",
    "\n",
    "%time cv_scores = cross_val_score(gnb_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hyperparameter tuning and output the best params and score\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# the 'priors' values come from the following:\n",
    "# - the first list of values is just indicating \n",
    "# equal weights between the 3 classes, which are 0, 1, and 2\n",
    "# - the second list of value refers to the weights\n",
    "# of the classes that we provided initially\n",
    "params = {#'priors': [[0.333, 0.333, 0.334], [0.55, 0.30, 0.15]],\n",
    "          'var_smoothing': [0,1e-3,1e-6, 1e-9,0.01,0.1,0.5,1]}\n",
    "\n",
    "gnb_gscv = GridSearchCV(gnb, params, cv=5)\n",
    "\n",
    "gnb_gscv.fit(X_train, y_train)\n",
    "\n",
    "preds_gnb = gnb_gscv.predict(X_test)\n",
    "accuracy_gnb_gscv = np.mean(y_test == preds_gnb)*100\n",
    "\n",
    "print('The accuracy for the refined Naive Bayes model is {}%'.format(accuracy_gnb_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in gnb_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined Naive Bayes model is {}%'.format(gnb_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_gnb_gscv = f1_score(y_test.values, preds_gnb, average='weighted')\n",
    "print('f1-score with grid search:', f1_gnb_gscv, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_gnb_gscv = confusion_matrix(y_test.values, preds_gnb)\n",
    "\n",
    "FP = cnf_matrix_gnb_gscv.sum(axis=0) - np.diag(cnf_matrix_gnb_gscv)  \n",
    "FN = cnf_matrix_gnb_gscv.sum(axis=1) - np.diag(cnf_matrix_gnb_gscv)\n",
    "TP = np.diag(cnf_matrix_gnb_gscv)\n",
    "TN = cnf_matrix_gnb_gscv.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_gnb_gscv, tnr_medrisk_gnb_gscv, tnr_lowrisk_gnb_gscv = TNR\n",
    "print(tnr_highrisk_gnb_gscv, tnr_medrisk_gnb_gscv, tnr_lowrisk_gnb_gscv)\n",
    "\n",
    "tnr_gnb_gscv = np.mean(TNR)\n",
    "print('average specificity:', tnr_gnb_gscv)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "gnb_best = GaussianNB(priors=[0.333, 0.333, 0.334], var_smoothing=0.1)\n",
    "gnb_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'Naive Bayes', gnb_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "gnb_best = GaussianNB(priors=[0.333, 0.333, 0.334], var_smoothing=0.1)\n",
    "gnb_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'Naive Bayes', gnb_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "preds_rf = rf.predict(X_test)\n",
    "accuracy_rf = rf.score(X_test, y_test)*100\n",
    "print('The accuracy for the Random Forest is {}%'.format(accuracy_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rf = f1_score(y_test.values, preds_rf, average='weighted')\n",
    "print('f1-score:', f1_rf, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_rf = confusion_matrix(y_test.values, preds_rf)\n",
    "\n",
    "FP = cnf_matrix_rf.sum(axis=0) - np.diag(cnf_matrix_rf)  \n",
    "FN = cnf_matrix_rf.sum(axis=1) - np.diag(cnf_matrix_rf)\n",
    "TP = np.diag(cnf_matrix_rf)\n",
    "TN = cnf_matrix_rf.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_rf, tnr_medrisk_rf, tnr_lowrisk_rf = TNR\n",
    "print(tnr_highrisk_rf, tnr_medrisk_rf, tnr_lowrisk_rf)\n",
    "\n",
    "tnr_rf = np.mean(TNR)\n",
    "print('average specificity:', tnr_rf)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_parameters = {'n_estimators':[100, 50, 10]}\n",
    "rf = RandomForestClassifier()\n",
    "clf5 = GridSearchCV(rf, rf_parameters)\n",
    "\n",
    "clf5.fit(X_train, y_train)\n",
    "preds_rf = clf5.predict(X_test)\n",
    "accuracy_rf2 = np.mean(y_test == preds_rf)*100\n",
    "\n",
    "print('The accuracy for the refined Random Forest model is {}%'.format(accuracy_rf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_rf, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-fold cross-validation\n",
    "rf_cv = RandomForestClassifier()\n",
    "\n",
    "%time cv_scores = cross_val_score(rf_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hyperparameter tuning and output the best params and score\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# the 'priors' values come from the following:\n",
    "# - the first list of values is the default, which just indicates \n",
    "# equal weights between the 3 classes, which are 0, 1, and 2\n",
    "# - the second list of value refers to the weights\n",
    "# of the classes that we provided initially\n",
    "params = {#'class_weight': [{0: 1, 1: 1, 2: 1}, {0: 0.55, 1: 0.30, 2: 0.15}],\n",
    "          'n_estimators':[200, 100, 50, 10]}\n",
    "\n",
    "rf_gscv = GridSearchCV(rf, params, cv=5)\n",
    "\n",
    "rf_gscv.fit(X_train, y_train)\n",
    "\n",
    "preds_rf = rf_gscv.predict(X_test)\n",
    "accuracy_rf_gscv = np.mean(y_test == preds_rf)*100\n",
    "\n",
    "print('The accuracy for the refined Random Forest model is {}%'.format(accuracy_rf_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in rf_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined Random Forest model is {}%'.format(rf_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rf_gscv = f1_score(y_test.values, preds_rf, average='weighted')\n",
    "print('f1-score with grid search:', f1_rf_gscv, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_rf_gscv = confusion_matrix(y_test.values, preds_rf)\n",
    "\n",
    "FP = cnf_matrix_rf_gscv.sum(axis=0) - np.diag(cnf_matrix_rf_gscv)  \n",
    "FN = cnf_matrix_rf_gscv.sum(axis=1) - np.diag(cnf_matrix_rf_gscv)\n",
    "TP = np.diag(cnf_matrix_rf_gscv)\n",
    "TN = cnf_matrix_rf_gscv.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_rf_gscv, tnr_medrisk_rf_gscv, tnr_lowrisk_rf_gscv = TNR\n",
    "print(tnr_highrisk_rf_gscv, tnr_medrisk_rf_gscv, tnr_lowrisk_rf_gscv)\n",
    "\n",
    "tnr_rf_gscv = np.mean(TNR)\n",
    "print('average specificity:', tnr_rf_gscv)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "rf_best = RandomForestClassifier(class_weight={0: 0.55, 1: 0.3, 2: 0.15}, \n",
    "                                 n_estimators=200)\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'Random Forest', rf_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "rf_best = RandomForestClassifier(class_weight={0: 0.55, 1: 0.3, 2: 0.15}, \n",
    "                                 n_estimators=200)\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'Random Forest', rf_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "preds_dt = dt.predict(X_test)\n",
    "accuracy_dt = dt.score(X_test, y_test)*100\n",
    "print('The accuracy for the Decision Tree model is {}%'.format(accuracy_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_dt = f1_score(y_test.values, preds_dt, average='weighted')\n",
    "print('f1-score:', f1_dt, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_dt = confusion_matrix(y_test.values, preds_dt)\n",
    "\n",
    "FP = cnf_matrix_dt.sum(axis=0) - np.diag(cnf_matrix_dt)  \n",
    "FN = cnf_matrix_dt.sum(axis=1) - np.diag(cnf_matrix_dt)\n",
    "TP = np.diag(cnf_matrix_dt)\n",
    "TN = cnf_matrix_dt.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_dt, tnr_medrisk_dt, tnr_lowrisk_dt = TNR\n",
    "print(tnr_highrisk_dt, tnr_medrisk_dt, tnr_lowrisk_dt)\n",
    "\n",
    "tnr_dt = np.mean(TNR)\n",
    "print('average specificity:', tnr_dt)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hyperparameter tuning and output the best params and score\n",
    "dt = DecisionTreeClassifier()\n",
    "params = {'class_weight': [{0: 1, 1: 1, 2: 1}, {0: 0.55, 1: 0.30, 2: 0.15}]}\n",
    "\n",
    "dt_gscv = GridSearchCV(dt, params, cv=5)\n",
    "\n",
    "dt_gscv.fit(X_train, y_train)\n",
    "\n",
    "preds_dt = dt_gscv.predict(X_test)\n",
    "accuracy_dt_gscv = np.mean(y_test == preds_dt)*100\n",
    "\n",
    "print('The accuracy for the refined Decision Tree model is {}%'.format(accuracy_dt_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in dt_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined Random Forest model is {}%'.format(rf_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_dt, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_dt_gscv = f1_score(y_test.values, preds_dt, average='weighted')\n",
    "print('f1-score with grid search:', f1_dt_gscv, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_dt_gscv = confusion_matrix(y_test.values, preds_dt)\n",
    "\n",
    "FP = cnf_matrix_dt_gscv.sum(axis=0) - np.diag(cnf_matrix_dt_gscv)  \n",
    "FN = cnf_matrix_dt_gscv.sum(axis=1) - np.diag(cnf_matrix_dt_gscv)\n",
    "TP = np.diag(cnf_matrix_dt_gscv)\n",
    "TN = cnf_matrix_dt_gscv.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('metric: specificity, with value:', TNR)\n",
    "\n",
    "tnr_highrisk_dt_gscv, tnr_medrisk_dt_gscv, tnr_lowrisk_dt_gscv = TNR\n",
    "print(tnr_highrisk_dt_gscv, tnr_medrisk_dt_gscv, tnr_lowrisk_dt_gscv)\n",
    "\n",
    "tnr_dt_gscv = np.mean(TNR)\n",
    "print('average specificity:', tnr_dt_gscv)\n",
    "\n",
    "# metrics = [TPR, TNR]\n",
    "# for metric in metrics:\n",
    "#     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "dt_best = DecisionTreeClassifier()\n",
    "dt_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'Decision Tree', dt_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "dt_best = DecisionTreeClassifier()\n",
    "dt_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'Decision Tree', dt_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network (Multi-Layer Perceptron (MLP) Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPClassifier()\n",
    "# mlp.fit(X_train, y_train)\n",
    "\n",
    "# preds_mlp = mlp.predict(X_test)\n",
    "# accuracy_mlp = mlp.score(X_test, y_test)*100\n",
    "# print('The accuracy for the MLP model is {}%'.format(accuracy_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_mlp = f1_score(y_test.values, preds_mlp, average='weighted')\n",
    "# print('f1-score:', f1_mlp, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnf_matrix_mlp = confusion_matrix(y_test.values, preds_mlp)\n",
    "\n",
    "# FP = cnf_matrix_mlp.sum(axis=0) - np.diag(cnf_matrix_mlp)  \n",
    "# FN = cnf_matrix_mlp.sum(axis=1) - np.diag(cnf_matrix_mlp)\n",
    "# TP = np.diag(cnf_matrix_mlp)\n",
    "# TN = cnf_matrix_mlp.sum() - (FP + FN + TP)\n",
    "\n",
    "# FP = FP.astype(float)\n",
    "# FN = FN.astype(float)\n",
    "# TP = TP.astype(float)\n",
    "# TN = TN.astype(float)\n",
    "\n",
    "# # Sensitivity, hit rate, recall, or true positive rate\n",
    "# TPR = TP/(TP+FN)\n",
    "# # Specificity or true negative rate\n",
    "# TNR = TN/(TN+FP) \n",
    "# # Precision or positive predictive value\n",
    "# PPV = TP/(TP+FP)\n",
    "# # Negative predictive value\n",
    "# NPV = TN/(TN+FN)\n",
    "# # Fall out or false positive rate\n",
    "# FPR = FP/(FP+TN)\n",
    "# # False negative rate\n",
    "# FNR = FN/(TP+FN)\n",
    "# # False discovery rate\n",
    "# FDR = FP/(TP+FP)\n",
    "# # Overall accuracy\n",
    "# ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "# print('metric: specificity, with value:', TNR)\n",
    "\n",
    "# tnr_highrisk_mlp, tnr_medrisk_mlp, tnr_lowrisk_mlp = TNR\n",
    "# print(tnr_highrisk_mlp, tnr_medrisk_mlp, tnr_lowrisk_mlp)\n",
    "\n",
    "# tnr_mlp = np.mean(TNR)\n",
    "# print('average specificity:', tnr_mlp)\n",
    "\n",
    "# # metrics = [TPR, TNR]\n",
    "# # for metric in metrics:\n",
    "# #     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform hyperparameter tuning and output the best params and score\n",
    "# mlp = MLPClassifier()\n",
    "# params = {'alpha':[1e-6, 0.0001, 0.1]}\n",
    "\n",
    "# mlp_gscv = GridSearchCV(mlp, params, cv=5)\n",
    "\n",
    "# mlp_gscv.fit(X_train, y_train)\n",
    "\n",
    "# preds_mlp = mlp_gscv.predict(X_test)\n",
    "# accuracy_mlp_gscv = np.mean(y_test == preds_mlp)*100\n",
    "\n",
    "# print('The accuracy for the refined MLP model is {}%'.format(accuracy_mlp_gscv))\n",
    "\n",
    "# # NOTE: the following commented-out code is to print\n",
    "# # what the best parameter values are and \n",
    "# # the mean cross-validated score of the best estimator\n",
    "# print('The best parameters are:')\n",
    "# for key, val in mlp_gscv.best_params_.items():\n",
    "#     print(str(key) + ':', val)\n",
    "# # print('\\nThe best score for the refined Random Forest model is {}%'.format(rf_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, preds_mlp, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_mlp_gscv = f1_score(y_test.values, preds_mlp, average='weighted')\n",
    "# print('f1-score with grid search:', f1_mlp_gscv, 'with average: weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnf_matrix_mlp_gscv = confusion_matrix(y_test.values, preds_mlp)\n",
    "\n",
    "# FP = cnf_matrix_mlp_gscv.sum(axis=0) - np.diag(cnf_matrix_mlp_gscv)  \n",
    "# FN = cnf_matrix_mlp_gscv.sum(axis=1) - np.diag(cnf_matrix_mlp_gscv)\n",
    "# TP = np.diag(cnf_matrix_mlp_gscv)\n",
    "# TN = cnf_matrix_mlp_gscv.sum() - (FP + FN + TP)\n",
    "\n",
    "# FP = FP.astype(float)\n",
    "# FN = FN.astype(float)\n",
    "# TP = TP.astype(float)\n",
    "# TN = TN.astype(float)\n",
    "\n",
    "# # Sensitivity, hit rate, recall, or true positive rate\n",
    "# TPR = TP/(TP+FN)\n",
    "# # Specificity or true negative rate\n",
    "# TNR = TN/(TN+FP) \n",
    "# # Precision or positive predictive value\n",
    "# PPV = TP/(TP+FP)\n",
    "# # Negative predictive value\n",
    "# NPV = TN/(TN+FN)\n",
    "# # Fall out or false positive rate\n",
    "# FPR = FP/(FP+TN)\n",
    "# # False negative rate\n",
    "# FNR = FN/(TP+FN)\n",
    "# # False discovery rate\n",
    "# FDR = FP/(TP+FP)\n",
    "# # Overall accuracy\n",
    "# ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "# print('metric: specificity, with value:', TNR)\n",
    "\n",
    "# tnr_highrisk_mlp_gscv, tnr_medrisk_mlp_gscv, tnr_lowrisk_mlp_gscv = TNR\n",
    "# print(tnr_highrisk_mlp_gscv, tnr_medrisk_mlp_gscv, tnr_lowrisk_mlp_gscv)\n",
    "\n",
    "# tnr_mlp_gscv = np.mean(TNR)\n",
    "# print('average specificity:', tnr_mlp_gscv)\n",
    "\n",
    "# # metrics = [TPR, TNR]\n",
    "# # for metric in metrics:\n",
    "# #     print('metric:', str(metric), 'with value:', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit a model with the best parameters\n",
    "# mlp_best = MLPClassifier()\n",
    "# mlp_best.fit(X_train, y_train)\n",
    "\n",
    "# # plot multiclass P-R curve\n",
    "# visualize_data.plot_precision_recall(\n",
    "#     'Multi-Layer Perceptron', mlp_best, X_test, y_test, \n",
    "#     n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit a model with the best parameters\n",
    "# mlp_best = MLPClassifier()\n",
    "# mlp_best.fit(X_train, y_train)\n",
    "\n",
    "# # plot multiclass ROC curve\n",
    "# visualize_data.plot_multiclass_roc(\n",
    "#     'Multi-Layer Perceptron', mlp_best, X_test, y_test, \n",
    "#     n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Logistic Regression', 'K Nearest Neighbors',\n",
    "         'Support Vector Machine', 'Naive Bayes', 'Random Forest', 'Decision Tree'] #'Multi-Layer Perceptron']\n",
    "model_accuracies = [accuracy_lg, accuracy_knn, \n",
    "                    accuracy_svc, accuracy_gnb, accuracy_rf, accuracy_dt] #accuracy_mlp]\n",
    "gscv_accuracies  = [accuracy_lr_gscv, accuracy_knn_gscv,\n",
    "                    accuracy_svc_gscv, accuracy_gnb_gscv, accuracy_rf_gscv, accuracy_dt_gscv] #accuracy_mlp_gscv]\n",
    "model_f1 = [f1_lr, f1_knn, f1_svc, f1_gnb, f1_rf, f1_dt] #f1_mlp]\n",
    "gscv_f1  = [f1_lr_gscv, f1_knn_gscv, f1_svc_gscv, f1_gnb_gscv, f1_rf_gscv, f1_dt_gscv] #f1_mlp_gscv]\n",
    "model_tnr = [tnr_lr, tnr_knn, tnr_svc, tnr_gnb, tnr_rf, tnr_dt] #tnr_mlp]\n",
    "gscv_tnr  = [tnr_lr_gscv, tnr_knn_gscv, tnr_svc_gscv, tnr_gnb_gscv, tnr_rf_gscv, tnr_dt_gscv] #tnr_mlp_gscv]\n",
    "model_comparison = pd.DataFrame(\n",
    "    np.column_stack([models, model_accuracies, gscv_accuracies, model_f1, gscv_f1, model_tnr, gscv_tnr]),\n",
    "    columns=['Model', 'Model Accuracy', 'Grid Search CV Accuracy', 'Model F1 Score', 'Grid Search CV F1 Score',\n",
    "            'Model Average Specificity', 'Grid Search CV Average Specificity'])\n",
    "data_types = ['float' for _ in range(len(model_comparison.columns)-1)]\n",
    "data_dict = dict(zip(model_comparison.columns[1:], data_types))\n",
    "model_comparison = model_comparison.astype(data_dict)\n",
    "model_comparison = model_comparison.round(2)\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best model for each metric\n",
    "# - if the best model for the metric is Support Vector Machine (SVM),\n",
    "#   then print the second best model to account for SVM's alleged unreliability\n",
    "# - else, print the actual best model\n",
    "for column in model_comparison.columns:\n",
    "    if column != 'Model':\n",
    "        print('metric:', column)\n",
    "        best_model = model_comparison.sort_values(column, ascending=False).reset_index(drop=True)['Model'][0]\n",
    "        if best_model == 'Support Vector Machine':\n",
    "            _2nd_best_model = model_comparison.sort_values(column, ascending=False).reset_index(drop=True)['Model'][1]\n",
    "            print('the second best model:', _2nd_best_model)\n",
    "        else:\n",
    "            print('the best model:', best_model)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
